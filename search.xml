<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[wikipedia 训练繁（简）体中文 embedding(word2vec)模型]]></title>
    <url>%2F2018%2F07%2F22%2Fwikipedia-train-traditional-chinese-embedding%EF%BC%88word2vec%EF%BC%89model%2F</url>
    <content type="text"><![CDATA[由于课题任务需要一个繁体中文的word3vec, 折腾经过记录在此。希望以后少掉几个坑。训练好的embedding放在网盘中， 密码：2um0后来又按照这个方法训练了简体中文维度分别为50、100、200、300的embedding，一并放出来网盘链接 密码：751d get wiki最新的wiki datas下载地址，目前有1.6G大小。 里面的内容以XML格式保存。节点信息如下：12345678&lt;page&gt; &lt;title&gt;&lt;/title&gt; &lt;id&gt;&lt;/id&gt; &lt;timestamp&gt;&lt;/timestamp&gt; &lt;username&gt;&lt;/username&gt; &lt;comment&gt;&lt;/comment&gt; &lt;text xml:space=&quot;preserve&quot;&gt;&lt;/text&gt;&lt;/page&gt; 初步处理我顺手直接解压真的too young。为了节省时间，免去自己写代码处理Wiki的烦恼，Wikipedia Extractor先初步处理。（服务器非root用户，安装命令加上--user） 1234git clone https://github.com/attardi/wikiextractor.git wikiextractorcd wikiextractorpython setup.py install --userpython WikiExtractor.py -b 1024M -o extracted zhwiki-latest-pages-articles.xml.bz2 执行过程如下，可以看到一共处理了1012693篇文章，输出如下所示： INFO: 6205533 手語新聞INFO: 6205536 班傑明·古根海姆INFO: 6205549 同意INFO: 6205556 2018年荷蘭網路監控法公民投票INFO: 6205594 李儒新INFO: 6205610 深圳信息职业技术学院INFO: 6205626 停下來等著你 (2018年電視劇)INFO: 6205642 簡單矩陣的快速演算法設計INFO: 6205644 斯義桂INFO: 6205646 焦耳效应INFO: 6205648 1925年世界大賽INFO: 6205653 True (方力申專輯)INFO: 6205657 华睿2号INFO: 6205664 河內郡 (大阪府)INFO: 6205691 京都寺町三条商店街的福爾摩斯INFO: 6205675 莫莉·比什死亡事件INFO: 6205703 都筑郡INFO: 6205701 皇座法庭所屬分庭庭長INFO: 6205709 冬瓜餅INFO: 6205710 吸血鬼莫比亞斯INFO: 6205712 淘綾郡INFO: 6205714 明石香織INFO: Finished 71-process extraction of 1012693 articles in 1114.1s (909.0 art/s) 通过以上抽取后得到两个文件wiki_00和wiki_01。里面的格式类似下面123&lt;doc id=&quot;5323477&quot; url=&quot;https://zh.wikipedia.org/wiki?curid=5323477&quot; title=&quot;結構與能動性&quot;&gt;文章内容&lt;/doc&gt; 在上面的基础上，我们在去掉一些不需要的特殊符号。12345678910111213141516import reimport sysimport codecsdef filte(input_file): p5 = re.compile(&apos;&lt;doc (.*)&gt;&apos;) p6 = re.compile(&apos;&lt;/doc&gt;&apos;) outfile = codecs.open(&apos;std_&apos; + input_file, &apos;w&apos;, &apos;utf-8&apos;) with codecs.open(input_file, &apos;r&apos;, &apos;utf-8&apos;) as myfile: for line in myfile: line = p5.sub(&apos;&apos;, line) line = p6.sub(&apos;&apos;, line) outfile.write(line) outfile.close()if __name__ == &apos;__main__&apos;:filte(input_file) input_file = sys.argv[1] 简体转繁体首先安装opencc-python网上一大堆教程，全是深坑！其实直接按照代码仓库作者的方法安装就好了。123git clone https://github.com/yichen0831/opencc-python.gitcd opencc-pythonpython setup.py install --user 但是，如果追求效率，可以安装opencc C++ 版本，python代码的效率堪忧。 看文档不难发现，繁体字也分为香港区和台湾省，要用怎么样的转换看具体需求就好12345678910from opencc import OpenCCopencc = OpenCC('s2hk')for filename in ['wiki_01','wiki_00']:truewith open('std_'+filename,'r',encoding='utf-8') as fin, open('hk_'+filename,'w',encoding='utf-8') as fou:truetruefor index , line in enumerate(fin.readlines()):truetruetruehk = opencc.convert(line)truetruetrueif index % 10000 == 0:truetruetruetrueprint(index,hk)truetruetruefou.write(hk) 得到了两个文件分别大小为 1024M 和154M jieba Segment先把两个wiki文件合并cat hk_wiki_00 hk_wiki_01 &gt; hk_wiki 1python -m jieba -d " " ./hk_wiki &gt; ./SegHk_wiki train word2vec运行下面写好的脚本，1234567891011121314151617# -*- coding: utf-8 -*-from gensim.models import word2vecfrom gensim.models import KeyedVectorsimport loggingimport osif not os.path.exists('./word2vec_tradiCN/'):trueos.makedirs('./word2vec_tradiCN/')logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)sentences = word2vec.LineSentence('./SegHk_wiki')for number in [50,100,200,300]:truemodel = word2vec.Word2Vec(sentences,size=number,window=5,min_count=5,workers=20)true# min-count 表示设置最低频率，默认为5，如果一个词语在文档中出现的次数小于该阈值，那么该词就会被舍弃; size代表词词向量的维度true# 为了后续建模读取vector方便，我们的保存格式应该和glove vector 保持一致truemodel.wv.save_word2vec_format('./word2vec_tradiCN/Wiki'+str(number)+'.txt', binary=False) 然而出现了Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.这个错误运行conda install nomkl安装nomkl，这是anaconda的问题。 test word2vec如果用python2，运行下面的测试脚本可能会出现如下错误KeyError: &quot;word &#39;\xe7\xb8\xbd\xe7\xb5\xb1&#39; not in vocabulary&quot;这个是python2对于中文的支持不太友好造成的，用python3即可表现正常。123456# -*- coding: utf-8 -*-from gensim.models.keyedvectors import KeyedVectorsfor number in [50,100,200,300]: wv = KeyedVectors.load_word2vec_format('./word2vec_tradiCN/Wiki'+ str(number)+'.txt', binary=False) print(number, wv.similarity('總統','民國')) #两个词的相关性 print(number, wv.most_similar(['倫敦','中國'],['北京']),'\n\n') # 北京is to中国 as 伦敦is to？ 注意，繁体中文，测试的时候也要用繁体的 这里直接测试了四组不同大小的embedding，可以对比效果。以这个简单的测试来说，200d embedding效果比较好。 当然，在实际中，效果怎么样，还是要实际测试。 50 0.0890698850 [(‘美國’, 0.8497380614280701), (‘英國’, 0.8156374096870422), (‘荷蘭’, 0.7635571956634521), (‘加拿大’, 0.7618951201438904), (‘蘇格蘭’, 0.7564111948013306), (‘法國’, 0.7498287558555603), (‘冰島’, 0.7447660565376282), (‘愛爾蘭’, 0.7290477752685547), (‘德國’, 0.7261558175086975), (‘哥倫比亞’, 0.715803861618042)] 100 0.0021609096100 [(‘英國’, 0.7518529891967773), (‘美國’, 0.716768741607666), (‘蘇格蘭’, 0.706271767616272), (‘德國’, 0.6398693323135376), (‘法國’, 0.6289862394332886), (‘愛爾蘭’, 0.6286278963088989), (‘荷蘭’, 0.6277433633804321), (‘英格蘭’, 0.625410795211792), (‘加拿大’, 0.6076068878173828), (‘威爾斯’, 0.6075741052627563)] 200 0.044366393200 [(‘英國’, 0.6959728598594666), (‘蘇格蘭’, 0.6404226422309875), (‘美國’, 0.6401909589767456), (‘英格蘭’, 0.6158463358879089), (‘愛爾蘭’, 0.5740842223167419), (‘德國’, 0.5558757781982422), (‘威爾斯’, 0.5539498925209045), (‘法國’, 0.5375431776046753), (‘荷蘭’, 0.5276069641113281), (‘威爾士’, 0.5051602721214294)] 300 0.034565542300 [(‘英國’, 0.6512337923049927), (‘蘇格蘭’, 0.5884094834327698), (‘英格蘭’, 0.5666802525520325), (‘美國’, 0.5420516729354858), (‘愛爾蘭’, 0.5202239751815796), (‘威爾斯’, 0.48060378432273865), (‘荷蘭’, 0.4763559103012085), (‘德國’, 0.4744102358818054), (‘法國’, 0.4675533175468445), (‘北愛爾蘭’, 0.46320733428001404)] 网盘链接训练好的四个embedding包含892594个词，都放到了网盘中，可以按需下载。 密码：2um0 参考文献word2vec实战：获取和预处理中文维基百科(Wikipedia)语料库，并训练成word2vec模型]]></content>
      <categories>
        <category>NLP</category>
        <category>Word2vex</category>
      </categories>
      <tags>
        <tag>Wikipedia</tag>
        <tag>Gensim</tag>
        <tag>Embedding</tag>
        <tag>Opencc</tag>
      </tags>
  </entry>
</search>
